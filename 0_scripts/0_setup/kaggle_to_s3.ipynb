{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "03f14520-a0e7-4707-bf8f-fd03cb2ca41b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Ingestion: Kaggle → S3\n",
    "Pulls raw datasets from Kaggle and writes them to S3 raw layer\n",
    "\n",
    "Sources: asaniczka/mountain-climbing-accidents-dataset, siddharth0935/himalayan-expeditions\n",
    "\n",
    "Destination: s3://secret/raw/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0d2ae921-9a5f-435c-8fe8-bf9970109b25",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ── INSTALL ──────────────────────────────────────────\n",
    "%pip install kagglehub[pandas-datasets]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b2cca4b1-76a4-4488-b2f4-21c1169b20dd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run /Workspace/Repos/nikum.vedansh@gmail.com/himalayan-expeditions-project/0_scripts/configs/credentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8aa9fbb0-849f-43af-ab0a-63aa69742d82",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run /Workspace/Repos/nikum.vedansh@gmail.com/himalayan-expeditions-project/0_scripts/configs/config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "27b3b38f-44c0-40ae-ab7a-7c3c43686755",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ── IMPORTS ──────────────────────────────────────────\n",
    "import kagglehub\n",
    "from kagglehub import KaggleDatasetAdapter\n",
    "\n",
    "import boto3\n",
    "from io import StringIO\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9189a45c-0ae5-4987-a747-c70db073c7a4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "s3_client = boto3.client(\n",
    "    's3',\n",
    "    aws_access_key_id=os.environ[\"AWS_ACCESS_KEY_ID\"],\n",
    "    aws_secret_access_key=os.environ[\"AWS_SECRET_ACCESS_KEY\"],\n",
    "    region_name=os.environ[\"AWS_DEFAULT_REGION\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "707ea248-3e3b-4478-ad89-53e0746d3bc9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def file_exists_in_s3(bucket, key):\n",
    "    try:\n",
    "        s3_client.head_object(Bucket=bucket, Key=key)\n",
    "        return True\n",
    "    except:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6f16d1f5-cb1b-40ee-b871-d4c6b0fafc55",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(\"Starting ingestion...\\n\")\n",
    "uploaded = []\n",
    "skipped = []\n",
    "failed = []\n",
    "\n",
    "for dataset in DATASETS:\n",
    "    try:\n",
    "        key = dataset[\"s3_path\"].replace(f\"s3://{S3_BUCKET}/\", \"\") + dataset[\"file\"]\n",
    "        \n",
    "        if file_exists_in_s3(S3_BUCKET, key):\n",
    "            skipped.append(dataset[\"file\"])\n",
    "            continue\n",
    "        \n",
    "        pandas_df = kagglehub.dataset_load(\n",
    "            KaggleDatasetAdapter.PANDAS,\n",
    "            dataset[\"kaggle_id\"],\n",
    "            dataset[\"file\"],\n",
    "            pandas_kwargs={\"encoding\": dataset[\"encoding\"]}\n",
    "        )\n",
    "        \n",
    "        if dataset[\"encoding\"] == \"latin1\":\n",
    "            pandas_df = pandas_df.apply(\n",
    "                lambda x: x.str.encode('latin1').str.decode('utf-8', errors='ignore') \n",
    "                if x.dtype == 'object' else x\n",
    "            )\n",
    "        \n",
    "        csv_buffer = StringIO()\n",
    "        pandas_df.to_csv(csv_buffer, index=False, encoding='utf-8')\n",
    "        \n",
    "        s3_client.put_object(\n",
    "            Bucket=S3_BUCKET,\n",
    "            Key=key,\n",
    "            Body=csv_buffer.getvalue()\n",
    "        )\n",
    "        \n",
    "        uploaded.append(dataset[\"file\"])\n",
    "\n",
    "    except Exception as e:\n",
    "        failed.append(dataset[\"file\"])\n",
    "        print(f\"❌ Failed {dataset['file']}: {str(e)}\")\n",
    "\n",
    "def print_summary(label, files):\n",
    "    print(f\"{label} ({len(files)}):\")\n",
    "    if files:\n",
    "        for f in files:\n",
    "            print(f\"   - {f}\")\n",
    "    else:\n",
    "        print(\"   - none\")\n",
    "    print()\n",
    "\n",
    "print_summary(\"✅ Uploaded\", uploaded)\n",
    "print_summary(\"⏩ Skipped\", skipped)\n",
    "print_summary(\"❌ Failed\",   failed)\n",
    "print(\"✅ Ingestion complete\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "kaggle_to_s3",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
