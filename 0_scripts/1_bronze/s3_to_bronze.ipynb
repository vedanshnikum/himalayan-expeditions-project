{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "74e41e64-fe45-49ba-afa0-7433e69a3b3e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "##S3 to bronze\n",
    "\n",
    "* Reads raw CSV files from S3 and writes them as Delta tables to himalaya.bronze\n",
    "\n",
    "* Source: s3://secret/raw/\n",
    "\n",
    "* Destination: himalaya.bronze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4b856e69-bb4f-49ba-9846-de04b8a72476",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run /Workspace/Repos/nikum.vedansh@gmail.com/himalayan-expeditions-project/0_scripts/configs/credentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "06649014-22d8-423a-a0fd-37637d031201",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run /Workspace/Repos/nikum.vedansh@gmail.com/himalayan-expeditions-project/0_scripts/configs/config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ce6f4a0f-9e87-4eaf-aa2c-83d583554149",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "import pandas as pd\n",
    "from io import StringIO\n",
    "import os\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f43602f8-16c2-4438-9db7-193db8cd4803",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "s3_client = boto3.client(\n",
    "    's3',\n",
    "    aws_access_key_id=os.environ[\"AWS_ACCESS_KEY_ID\"],\n",
    "    aws_secret_access_key=os.environ[\"AWS_SECRET_ACCESS_KEY\"],\n",
    "    region_name=os.environ[\"AWS_DEFAULT_REGION\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f777bf22-7295-4bd8-b284-a7851a234248",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(\"Starting S3 → Bronze ingestion...\\n\")\n",
    "loaded = []\n",
    "skipped = []\n",
    "failed = []\n",
    "\n",
    "for dataset in DATASETS:\n",
    "    try:\n",
    "        table = dataset[\"bronze_name\"]\n",
    "        \n",
    "        # Skip if table already exists\n",
    "        if spark.catalog.tableExists(table):\n",
    "            skipped.append(table)\n",
    "            continue\n",
    "        \n",
    "        # Read from S3\n",
    "        key = dataset[\"s3_path\"].replace(f\"s3://{S3_BUCKET}/\", \"\") + dataset[\"file\"]\n",
    "        obj = s3_client.get_object(Bucket=S3_BUCKET, Key=key)\n",
    "        pandas_df = pd.read_csv(obj[\"Body\"], encoding=\"utf-8\")\n",
    "        \n",
    "        # Add ingestion timestamp\n",
    "        pandas_df[\"ingested_at\"] = datetime.now()\n",
    "        \n",
    "        # Convert to Spark and write to Delta\n",
    "        spark_df = spark.createDataFrame(pandas_df)\n",
    "        spark_df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(table)\n",
    "        \n",
    "        loaded.append(table)\n",
    "\n",
    "    except Exception as e:\n",
    "        failed.append(dataset[\"bronze_name\"])\n",
    "        print(f\"❌ Failed {dataset['bronze_name']}: {str(e)}\")\n",
    "\n",
    "def print_summary(label, items):\n",
    "    print(f\"{label} ({len(items)}):\")\n",
    "    if items:\n",
    "        for i in items:\n",
    "            print(f\"   - {i}\")\n",
    "    else:\n",
    "        print(\"   - none\")\n",
    "    print()\n",
    "\n",
    "print_summary(\"✅ Loaded\", loaded)\n",
    "print_summary(\"⏭  Skipped\", skipped)\n",
    "print_summary(\"❌ Failed\", failed)\n",
    "print(\"✓ Bronze ingestion complete\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "s3_to_bronze",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
