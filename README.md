# ğŸ”ï¸ Himalayan Expeditions Data Pipeline

## Overview

This project explores how weather conditions have historically influenced climbing outcomes in the Himalayas. Using decades of expedition records, fatality data, and weather data across major Himalayan peaks, the pipeline analyzes patterns between conditions and climbing success or failure.

Incremental weather data is simulated by uploading historic weather files in batches â€” mimicking how a real pipeline would ingest newly available data over time. Comparisons are made against historical baselines to assess how dangerous conditions were during past climbs.

> **Note on predictions:** No machine learning is used. Analysis is pattern-based comparison against historic data only.

> **Note on incremental data:** As live Himalayan weather APIs are not publicly available, historic weather data past a cutoff date is uploaded incrementally to simulate a real-time ingestion pattern.

---

## ğŸ› ï¸ Tech Stack

| Tool | Purpose |
|---|---|
| Python / PySpark | Pipeline logic and data transformation |
| SQL | Querying and table creation |
| AWS S3 | Raw data lake and object storage |
| Databricks Community Edition | ELT processing and dashboarding |
| Delta Lake | ACID-compliant table storage |
| Databricks Dashboard | BI reporting and visualisation |
| Databricks Genie | Natural language querying |

---

## ğŸ—ï¸ Architecture

This project follows a **Data Lakehouse** pattern with **Medallion Architecture** (Bronze â†’ Silver â†’ Gold).
```
Kaggle API
    â†“
AWS S3 (raw layer)
    â†“
Bronze (raw Delta tables)
    â†“
Silver (cleaned and joined)
    â†“
Gold (aggregated, dashboard-ready)
    â†“
Databricks Dashboard
```

---

## ğŸ“‚ Repository Structure
```
himalayan-expeditions-project/
  0_code/
    0_setup/
      catalog_setup
      data_upload        â† Kaggle â†’ S3 ingestion notebook
    other/
      config             â† S3 paths and dataset configuration
      credentials        â† API keys (not pushed to GitHub)
  dictionary/
    data_dictionary      â† Column reference notebook
  LICENSE
  README.md
  .gitignore
```

---

## ğŸ“Š Data Sources

| Dataset | Author | Last Updated |
|---|---|---|
| [Mountain Climbing Accidents Dataset](https://www.kaggle.com/datasets/asaniczka/mountain-climbing-accidents-dataset) | asaniczka | 2 years ago |
| [Historic Weather Data for Himalayan Peaks](https://www.kaggle.com/datasets/bonesclarke26/historic-weather-data-for-himalayan-peaks) | Ryan Clarke | 7 months ago |
| [Himalayan Expeditions](https://www.kaggle.com/datasets/siddharth0935/himalayan-expeditions) | Siddharth Vora | 9 months ago |
| [Mount Everest Accident Dataset 2020â€“2025](https://www.kaggle.com/datasets/syedmuhammadbilal12/mount-everest-accident-dataset-2020-2025) | Syed Muhammad Bilal | 2 months ago |

---

## ğŸ”„ Pipeline Stages

### âœ… Stage 1 â€” Ingestion (Kaggle â†’ S3)
Raw datasets pulled from Kaggle using `kagglehub` and written to AWS S3 as UTF-8 encoded CSV files. Encoding inconsistencies handled at ingestion. Idempotent â€” skips files that already exist in S3.

### ğŸ”„ Stage 2 â€” Bronze *(in progress)*
Raw files read from S3 and written to Delta tables in Databricks Unity Catalog with minimal transformation. Ingestion timestamp added to each record.

### â¬œ Stage 3 â€” Silver *(upcoming)*
Data cleaned, typed, deduplicated, and joined into a unified analytical layer.

### â¬œ Stage 4 â€” Gold *(upcoming)*
Aggregated tables built for the dashboard â€” survival rates by peak, season, weather conditions, and expedition type.

### â¬œ Stage 5 â€” Dashboard *(upcoming)*
Interactive visualisation and natural language querying via Databricks Dashboard and Genie.

---

## ğŸ’¡ Key Design Decisions

### ELT over ETL
Data is loaded to S3 raw before any transformation occurs. This means raw data is always preserved and transformations can be rerun at any time without re-extracting from Kaggle. If transformation logic changes at any stage, the raw layer is untouched and acts as a reliable source of truth.

### IAM over Root
A dedicated IAM user was created for this project rather than using the AWS root account. Root credentials should never be used in application code. The IAM user has scoped S3 permissions, meaning if the credentials were ever compromised, the blast radius is limited to this project only.

### Encoding standardised at ingestion
`refer.csv` contained Latin-1 encoded characters causing codec errors on read. Rather than handling this in Silver or later, the re-encoding to UTF-8 happens immediately at ingestion before the file lands in S3. This means every layer downstream â€” Bronze, Silver, Gold â€” works with consistent UTF-8 data and encoding is never a concern again.

### Idempotent ingestion
The Kaggle to S3 ingestion notebook checks whether a file already exists in S3 before uploading. If it does, it is skipped. This means the notebook can be rerun at any time without duplicating data â€” important for a pipeline that may be triggered multiple times.

### Config separated from code
All dataset paths, Kaggle IDs, and S3 configuration live in a dedicated `config` notebook. This means the ingestion logic never needs to change if a path or source changes â€” only the config does. Credentials are stored separately and excluded from version control entirely.

---

## âš™ï¸ Setup

1. Clone the repo
2. Create a `credentials` notebook in `0_code/other/` with your Kaggle and AWS keys
3. Update `config` notebook with your S3 bucket name
4. Run `0_code/0_setup/data_upload` to ingest raw data into S3